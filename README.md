
# ğŸ“˜ Vehicle Specification Extraction System

*A Retrieval-Augmented Generation (RAG) Pipeline for Extracting Automotive Specifications from Service Manuals*
**Developed for: Predii India Private Limited Assignment**

---

# ğŸ“Œ **1. Project Overview**

This project implements a **complete end-to-end pipeline** that extracts **vehicle specifications**â€”such as torque values, fluid capacities, and part numbersâ€”from an automotive service manual in PDF format.

The system uses:

* **PDF Parsing**
* **Text Preprocessing**
* **Semantic Chunking**
* **Vector Embeddings**
* **FAISS Vector Search**
* **Retrieval-Augmented Generation (RAG) using an LLM**
* **Structured JSON Output**

This tool allows a user to input a query like:

> â€œTorque for brake caliper boltsâ€

and receive precise structured results such as:

```json
[
  {
    "component": "Brake Caliper Bolt",
    "spec_type": "Torque",
    "value": "35",
    "unit": "Nm",
    "page_number": 72
  }
]
```

This pipeline follows exactly the guidelines provided in the assignment.

---

# ğŸ“ **2. Folder Structure**

```
vehicle-spec-extraction/
â”‚
â”œâ”€ data/
â”‚   â””â”€ service_manual.pdf
â”‚
â”œâ”€ src/
â”‚   â”œâ”€ pdf_loader.py
â”‚   â”œâ”€ preprocess.py
â”‚   â”œâ”€ chunker.py
â”‚   â”œâ”€ embed_store.py
â”‚   â”œâ”€ retriever.py
â”‚   â”œâ”€ llm_extractor.py
â”‚   â”œâ”€ postprocess.py
â”‚   â””â”€ __init__.py
â”‚
â”œâ”€ outputs/
â”‚   â””â”€ specs.json
â”‚
â”œâ”€ notebooks/
â”‚
â”œâ”€ run_pipeline.py
â”œâ”€ requirements.txt
â””â”€ README.md
```

---

# âš™ï¸ **3. Features**

### âœ” PDF Parsing

Extracts raw text from automotive service manuals using **PyMuPDF**.

### âœ” Preprocessing

Cleans text, removes noise, normalizes spaces & hyphenated line breaks.

### âœ” Intelligent Chunking

Splits content into overlapping semantic chunks for improved retrieval.

### âœ” Embedding + Vector Store

Uses **Sentence Transformers** + **FAISS** for efficient similarity search.

### âœ” Retrieval-Augmented Generation (RAG)

Feeds relevant chunks to an LLM for correct, context-backed extraction.

### âœ” Structured Output

Outputs specifications in clean **JSON** format.

---

# ğŸš€ **4. How to Run the Project**

## **Step 1: Clone or Create Project Folder**

```bash
git clone <repo-url>
```

Or create your folder manually.

---

## **Step 2: Create and Activate Virtual Environment**

Windows:

```bash
python -m venv venv
venv\Scripts\activate
```

Linux/Mac:

```bash
python3 -m venv venv
source venv/bin/activate
```

---

## **Step 3: Install Requirements**

Inside your project folder:

```bash
pip install -r requirements.txt
```

---

## **Step 4: Add Your PDF**

Place the service manual inside:

```
data/service_manual.pdf
```

---

## **Step 5: Add Your OpenAI API Key**

Create `.env` file:

```
GOOGLE_API_KEY=your_api_key_here
```

---

## **Step 6: Run the Entire Pipeline**

```bash
python run_pipeline.py
```

This performs:

* PDF Extraction
* Chunking
* Embedding
* Vector Index Building
* Querying
* LLM Extraction

---

## **Step 7: View Final Output**

Output location:

```
outputs/specs.json
```

---

# ğŸ§  **5. System Architecture**

Below is the step-by-step pipeline followed:

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     Service Manual PDF     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   PDF Extraction   â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚    Preprocessing   â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚      Chunking      â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚    Embeddings      â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚    FAISS Index     â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   Query Retrieval  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  LLM Extraction    â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  JSON Output File  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ”§ **6. Tools & Technologies Used**

| Component     | Technology                                      |
| ------------- | ----------------------------------------------- |
| PDF Parser    | PyMuPDF                                         |
| Text Cleaning | Python                                   |
| Embeddings    | SentenceTransformers (MiniLM-L6-v2)             |
| Vector Store  | FAISS CPU                                       |
| LLM           | gemini-2.5-flash                                |
| Output        | JSON                                            |

---

# ğŸ“ **7. Design Decisions**

### âœ” Why PyMuPDF?

Fast, accurate text extraction that preserves formatting better than pdfminer.

### âœ” Why Sentence Transformers?

Very fast + high-quality semantic embeddings.

### âœ” Why FAISS?

Industry-standard for similarity search and vector retrieval.

### âœ” Why Chunk Overlap?

Avoids losing important contextual information.

### âœ” Why JSON Output?

Matches assignment requirement and ideal for API integration.

---

# ğŸ“ˆ **8. Ideas for Future Improvements**

These can be mentioned during your interview:

* Add **OCR** for image-based PDFs (Tesseract or EasyOCR)
* Add **Streamlit UI** for better user experience
* Implement **multi-query search** for complex cases
* Improve chunking using **heading-based section segmentation**
* Add unit normalization (Nm â†” ft-lb)
* Add accuracy evaluation by manual ground truthing

---

# ğŸ™‹â€â™‚ï¸ **9. Maintainer**

**Developer:** Yashvant Saroj
**Assignment:** Predii India Private Limited â€“ LLM Specification Extraction Task
**Year:** 2025

---




